{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import utils\n",
    "from nltk.tree import Tree\n",
    "from nli_rnn import ClassifierRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_sample_src = os.path.join('nli-data', 'snli_1.0_cs224u_sample.pickle')\n",
    "\n",
    "# Load the dataset: a dict with keys `train`, `dev`, and `vocab`. The first\n",
    "# two are lists of `dict`s sampled from the SNLI JSONL files. The third is\n",
    "# the complete vocabulary of the leaves in the trees for `train` and `dev`.\n",
    "snli_sample = pickle.load(open(snli_sample_src, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "devs = []\n",
    "with open('snli_1.0/snli_1.0_dev.jsonl') as f:\n",
    "    for line in f:\n",
    "        j = json.loads(line)\n",
    "        devs.append(j)\n",
    "        \n",
    "trains = []\n",
    "with open('snli_1.0/snli_1.0_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        j = json.loads(line)\n",
    "        trains.append(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snli = {}\n",
    "snli['dev'] = devs\n",
    "snli['train'] = trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "import nltk\n",
    "# exclude = set(string.punctuation)\n",
    "\n",
    "# an alternative method: still doesn't seem to give enough vocab. below (somewhere) i just take the vocab from the\n",
    "# build_rnn_dataset output\n",
    "vc = nltk.word_tokenize(' '.join([sentence['sentence1']+ ' '+sentence['sentence2'] for sentence in (snli['train'] + snli['dev'])]))\n",
    "\n",
    "# vc = set()\n",
    "# for sentence in snli['train'][:10000]:\n",
    "#     things = sentence['sentence1']\n",
    "# #     for thing in things:\n",
    "# #         thing = ''.join(ch for ch in thing if ch not in exclude)\n",
    "# #         vc.add(thing)\n",
    "#     \n",
    "# #     for thing in nltk.word_tokenize(things):\n",
    "# #         vc.add(thing)\n",
    "# # #     for thing in things:\n",
    "# # #         thing = ''.join(ch for ch in thing if ch not in exclude)\n",
    "# # #         vc.add(thing)\n",
    "# #     for thing in nltk.word_tokenize(things):\n",
    "# #         vc.add(thing)\n",
    "        \n",
    "\n",
    "snli['vocab'] = list(set(vc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-d216c3510a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print len(snli_sample['train'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print snli['vocab']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr2tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence2_binary_parse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "\n",
    "print (str2tree(snli['train'][0]['sentence2_binary_parse'])).subtrees()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snli_reader(sample):\n",
    "    \"\"\"Reader for SNLI data. `sample` just needs to be an iterator over\n",
    "    the SNLI JSONL files. For this notebook, it will always be \n",
    "    `snli_sample`, but, for example, the following should work for the \n",
    "    corpus files:\n",
    "    \n",
    "    import json    \n",
    "    def sample(src_filename):\n",
    "        for line in open(src_filename):\n",
    "            yield json.loads(line)\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        (tree1, tree2, label), where the trees are from `str2tree` and\n",
    "        label is in `LABELS` above.\n",
    "      \n",
    "    \"\"\"\n",
    "    for d in sample:\n",
    "        yield (str2tree(d['sentence1_parse']), \n",
    "               str2tree(d['sentence2_parse']),\n",
    "               d['gold_label'])\n",
    "        \n",
    "def train_reader():\n",
    "    \"\"\"Convenience function for reading just the training data.\"\"\"\n",
    "    return snli_reader(snli['train'])\n",
    "\n",
    "def dev_reader():\n",
    "    \"\"\"Convenience function for reading just the dev data.\"\"\"\n",
    "    return snli_reader(snli['dev'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2tree(s):\n",
    "    \"\"\"Map str `s` to an `nltk.tree.Tree` instance. The assumption is that \n",
    "    `s` represents a standard Penn-style tree.\"\"\"\n",
    "    return Tree.fromstring(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn_dataset(reader):\n",
    "    \"\"\"Build RNN datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader\n",
    "        SNLI iterator like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        The first member of each tuple is a list of strings (the\n",
    "        concatenated leaves) and the second is an np.array \n",
    "        (dimension 3) with a single 1 for the true class and 0s\n",
    "        in the other two positions\n",
    "       \n",
    "    \"\"\"    \n",
    "    dataset = []\n",
    "    for (t1, t2, label) in reader():\n",
    "        seq = t1.leaves() + t2.leaves()\n",
    "        y_ = np.zeros(3)\n",
    "        if label == '-':\n",
    "            label = 'neutral'\n",
    "        y_[LABELS.index(label)] = 1.0\n",
    "        dataset.append((seq, y_))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model_evaluation(mod, assess, labels=LABELS):\n",
    "    \"\"\"Asssess a trained `ClassifierRNN`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mod : `ClassifierRNN`\n",
    "        Should be a model trained on data in the same format as\n",
    "        `assess`.\n",
    "    \n",
    "    assess : list\n",
    "        A list of (seq, label) pairs, where seq is a sequence of\n",
    "        words and label is a one-hot vector giving the label.        \n",
    "    \n",
    "    \"\"\"    \n",
    "    # Assessment:\n",
    "    gold = []\n",
    "    predictions = []    \n",
    "    for seq, y_ in assess:\n",
    "        # The gold labels are vectors. Get the index of the single 1\n",
    "        # and look up its string in `LABELS`:\n",
    "        gold.append(labels[np.argmax(y_)])\n",
    "        # `predict` returns the index of the highest score.\n",
    "        p = mod.predict(seq) \n",
    "        predictions.append(labels[p])\n",
    "    # Report:\n",
    "    return classification_report(gold, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_experiment(\n",
    "        train_inp,\n",
    "        dev_inp,\n",
    "        vocab, \n",
    "        embedding, \n",
    "        hidden_dim, \n",
    "        eta, \n",
    "        maxiter):\n",
    "    \"\"\"Classifier RNN experiments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : list of str\n",
    "        Must contain every word we'll encounter in training or assessment.\n",
    "        \n",
    "    embedding : np.array\n",
    "        Embedding matrix for `vocab`. The ith row gives the input \n",
    "        representation for the ith member of vocab. Thus, `embedding`\n",
    "        must have the same row count as the length of vocab. Its\n",
    "        columns can be any length. (That is, the input word \n",
    "        representations can be any length.)\n",
    "        \n",
    "    hidden_dim : int (default: 10)\n",
    "        Dimensionality of the hidden representations. This is a\n",
    "        parameter to `ClassifierRNN`.\n",
    "        \n",
    "    eta : float (default: 0.05)\n",
    "        The learning rate. This is a parameter to `ClassifierRNN`.       \n",
    "        \n",
    "    maxiter : int (default: 10)\n",
    "        Maximum number of training epochs. This is a parameter \n",
    "        to `ClassifierRNN`.       \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted `sklearn` `classification_report`.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Training:\n",
    "    train = build_rnn_dataset(train_inp)       \n",
    "    mod = ClassifierRNN(\n",
    "        vocab, \n",
    "        embedding, \n",
    "        hidden_dim=hidden_dim, \n",
    "        eta=eta,\n",
    "        maxiter=maxiter)\n",
    "    mod.fit(train)    \n",
    "    # Assessment:\n",
    "    assess = build_rnn_dataset(dev_inp) \n",
    "    return rnn_model_evaluation(mod, assess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc = set()\n",
    "\n",
    "for x in build_rnn_dataset(train_reader):\n",
    "    vc = vc.union(set(x[0]))\n",
    "        \n",
    "for x in build_rnn_dataset(dev_reader):\n",
    "    vc = vc.union(set(x[0]))\n",
    "        \n",
    "snli['vocab'] = list(vc)\n",
    "# vc = set()\n",
    "# print [x[0] for x in build_rnn_dataset(train_reader)[:5]]\n",
    "# a = [[u'A', u'person', u'on', u'a', u'horse', u'jumps', u'over', u'a', u'broken', u'down', u'airplane', u'.', u'A', u'person', u'is', u'training', u'his', u'horse', u'for', u'a', u'competition', u'.'], [u'A', u'person', u'on', u'a', u'horse', u'jumps', u'over', u'a', u'broken', u'down', u'airplane', u'.', u'A', u'person', u'is', u'at', u'a', u'diner', u',', u'ordering', u'an', u'omelette', u'.'], [u'A', u'person', u'on', u'a', u'horse', u'jumps', u'over', u'a', u'broken', u'down', u'airplane', u'.', u'A', u'person', u'is', u'outdoors', u',', u'on', u'a', u'horse', u'.'], [u'Children', u'smiling', u'and', u'waving', u'at', u'camera', u'They', u'are', u'smiling', u'at', u'their', u'parents'], [u'Children', u'smiling', u'and', u'waving', u'at', u'camera', u'There', u'are', u'children', u'present']]\n",
    "# for x in a:\n",
    "#     print x\n",
    "#     vc = vc.union(x)\n",
    "# print vc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finished epoch 1 of 1; error is 1.2139977093"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.33      0.50      0.40      3278\n",
      " entailment       0.33      0.50      0.40      3329\n",
      "    neutral       0.00      0.00      0.00      3393\n",
      "\n",
      "avg / total       0.22      0.33      0.26     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = snli['vocab']\n",
    "# for seq,labels in training_data:\n",
    "#     print 1\n",
    "#     break\n",
    "# vocab = snli_sample['vocab']\n",
    "# Random embeddings of dimension 10:\n",
    "randvec_embedding = np.array([utils.randvec(10) for w in vocab])\n",
    "\n",
    "# A small network, trained for just a few epochs to see how things look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "def train_reader_sample():\n",
    "    return snli_reader(snli_sample['train'])\n",
    "def dev_reader_sample(): \n",
    "    return snli_reader(snli_sample['dev'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "glove_home = '/Users/reuben/Documents/glove.6B'\n",
    "GLOVE = utils.glove2dict(os.path.join(glove_home, 'glove.6B.50d.txt'))\n",
    "\n",
    "\n",
    "def build_glove_embedding(vocab):\n",
    "    return np.array([GLOVE[x] if x in GLOVE else utils.randvec(50) for x in vocab ])\n",
    "\n",
    "print 1\n",
    "glove_embedding = build_glove_embedding(snli_sample['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 1.11585145787"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.50      0.01      0.01      3278\n",
      " entailment       0.33      0.98      0.50      3329\n",
      "    neutral       0.46      0.03      0.06      3393\n",
      "\n",
      "avg / total       0.43      0.34      0.19     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print rnn_experiment(train_reader,\n",
    "                     dev_reader,\n",
    "                     snli['vocab'], \n",
    "                     build_glove_embedding(snli['vocab']), \n",
    "                     hidden_dim=10, \n",
    "                     eta=0.01, \n",
    "                     maxiter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cross_product_phi(t1, t2):\n",
    "    \"\"\"Basis for cross-product features. This tends to produce pretty \n",
    "    dense representations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1, t2 : `nltk.tree.Tree`\n",
    "        As given by `str2tree`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "        Maps each (w1, w2) in the cross-product of `t1.leaves()` and \n",
    "        `t2.leaves()` to its count. This is a multi-set cross-product\n",
    "        (repetitions matter).\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter([(w1, w2) for w1, w2 in itertools.product(t1.leaves(), t2.leaves())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_linear_classifier_dataset(\n",
    "        reader,\n",
    "        phi, \n",
    "        vectorizer=None):\n",
    "    \"\"\"Create a dataset for training classifiers using `sklearn`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader\n",
    "        An SNLI iterator like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : feature function\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    vectorizer : `sklearn.feature_extraction.DictVectorizer`   \n",
    "        If this is None, then a new `DictVectorizer` is created and\n",
    "        used to turn the list of dicts created by `phi` into a \n",
    "        feature matrix. This happens when we are training.\n",
    "              \n",
    "        If this is not None, then it's assumed to be a `DictVectorizer` \n",
    "        and used to transform the list of dicts. This happens in \n",
    "        assessment, when we take in new instances and need to \n",
    "        featurize them as we did in training.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict with keys 'X' (the feature matrix), 'y' (the list of\n",
    "        labels), 'vectorizer' (the `DictVectorizer`), and \n",
    "        'raw_examples' (the original tree pairs, for error analysis).\n",
    "    \n",
    "    \"\"\"\n",
    "    feat_dicts = []\n",
    "    labels = []\n",
    "    raw_examples = []\n",
    "    for t1, t2, label in reader():\n",
    "        d = phi(t1, t2)\n",
    "        feat_dicts.append(d)\n",
    "        labels.append(label)   \n",
    "        raw_examples.append((t1, t2))\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    else:\n",
    "        feat_matrix = vectorizer.transform(feat_dicts)\n",
    "    return {'X': feat_matrix, \n",
    "            'y': labels, \n",
    "            'vectorizer': vectorizer, \n",
    "            'raw_examples': raw_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `sklearn.linear.model.LogisticRegression`. This is also \n",
    "    called a Maximum Entropy (MaxEnt) Classifier, which is more fitting \n",
    "    for the multiclass case.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.\n",
    "        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    `sklearn.linear.model.LogisticRegression`\n",
    "        A trained `LogisticRegression` instance.\n",
    "    \n",
    "    \"\"\"\n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_classifier_experiment(\n",
    "        train_reader,\n",
    "        assess_reader,\n",
    "        phi,\n",
    "        train_func=fit_maxent_classifier):  \n",
    "    \"\"\"Runs experiments on our SNLI fragment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_reader, assess_reader\n",
    "        SNLI iterators like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : feature function (default: `word_overlap_phi`)\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    train_func : model wrapper (default: `fit_maxent_classifier`)\n",
    "        Any function that takes a feature matrix and a label list\n",
    "        as its values and returns a fitted model with a `predict`\n",
    "        function that operates on feature matrices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted `classification_report` from `sklearn`.\n",
    "        \n",
    "    \"\"\"\n",
    "    train = build_linear_classifier_dataset(train_reader, phi)\n",
    "    assess = build_linear_classifier_dataset(assess_reader, phi, vectorizer=train['vectorizer'])\n",
    "    print 1\n",
    "\n",
    "    mod = fit_maxent_classifier(train['X'], train['y'])\n",
    "    predictions = mod.predict(assess['X'])\n",
    "    return classification_report(assess['y'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-746e2cd07d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_classifier_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_reader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_cross_product_phi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-247-5faa0aa25448>\u001b[0m in \u001b[0;36mlinear_classifier_experiment\u001b[0;34m(train_reader, assess_reader, phi, train_func)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_maxent_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-235-4fbaad199511>\u001b[0m in \u001b[0;36mfit_maxent_classifier\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reuben/Downloads/ENTER/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 self.max_iter, self.tol, self.random_state)\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reuben/Downloads/ENTER/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         epsilon)\n\u001b[0m\u001b[1;32m    917\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(linear_classifier_experiment(train_reader,dev_reader,word_cross_product_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
