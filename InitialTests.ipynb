{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/sparsetools/__init__.py:3: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/sparsetools/_graph_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._graph_tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/sparsetools/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._traversal import connected_components\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import _hashing\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/extmath.py:20: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/extmath.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .sparsefuncs_fast import csr_row_norms\n",
      "/usr/local/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import vonmises_cython\n",
      "/usr/local/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:24: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
      "  from . import vonmises_cython\n",
      "/usr/local/lib/python2.7/site-packages/scipy/stats/stats.py:188: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._rank import rankdata, tiecorrect\n",
      "/usr/local/lib/python2.7/site-packages/scipy/stats/stats.py:188: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
      "  from ._rank import rankdata, tiecorrect\n",
      "/usr/local/lib/python2.7/site-packages/scipy/interpolate/interpolate.py:28: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import _ppoly\n",
      "/usr/local/lib/python2.7/site-packages/scipy/interpolate/interpolate.py:28: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
      "  from . import _ppoly\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:90: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .ckdtree import *\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:90: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
      "  from .ckdtree import *\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .qhull import *\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
      "  from .qhull import *\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ..utils import array2d, arrayfuncs, as_float_array, check_arrays\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:56: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import cd_fast\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/__init__.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import libsvm, liblinear\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/svm/base.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import libsvm_sparse\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/random.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._random import sample_without_replacement\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import utils\n",
    "from nltk.tree import Tree\n",
    "from nli_rnn import ClassifierRNN\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TfNeuralNetwork:\n",
    "    \"\"\"Fairly exact reproduction of `ShallowNeuralNetwork` in\n",
    "    TensorFlow, differing only in some details of optimization.\"\"\"\n",
    "    def __init__(self, hidden_dim=40, maxiter=100, eta=0.05):\n",
    "        \"\"\"All the parameters are set as attributes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_dim : int (default: 40)\n",
    "            Dimensionality of the hidden layer.                   \n",
    "\n",
    "        maxiter : int default: 100)\n",
    "            Maximum number of training epochs.\n",
    "            \n",
    "        eta : float (default: 0.05)\n",
    "            Learning rate.                 \n",
    "        \n",
    "        \"\"\"\n",
    "        self.input_dim = None\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = None\n",
    "        self.maxiter = maxiter\n",
    "        self.eta = eta\n",
    "        self.loss_hist = [] \n",
    "\n",
    "    def get_hist(self):\n",
    "        return self.loss_hist\n",
    "                \n",
    "    def fit(self, training_data):\n",
    "        \"\"\"The training algorithm. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data : list\n",
    "            A list of (example, label) pairs, where `example`\n",
    "            and `label` are both np.array instances.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.sess : the TensorFlow session\n",
    "        self.x : place holder for input data\n",
    "        self.h : the hidden layer\n",
    "        self.y : the output layer -- more like the full model here.\n",
    "        self.W1 : dense weight connection from self.x to self.h\n",
    "        self.b1 : bias\n",
    "        self.W2 : dense weight connection from self.h to self.y\n",
    "        self.b2 : bias\n",
    "        self.y_ : placeholder for training data\n",
    "                \n",
    "        \"\"\"\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        # Dimensions determined by the data:\n",
    "        self.input_dim = len(training_data[0][0])\n",
    "        self.output_dim = len(training_data[0][1])\n",
    "        # Network initialization. For the inputs x, None in the first\n",
    "        # dimension allows us to train and evaluate on datasets\n",
    "        # of different size.\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "        self.W1 = tf.Variable(tf.random_normal([self.input_dim, self.hidden_dim]))\n",
    "        self.b1 = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
    "        self.W2 = tf.Variable(tf.random_normal([self.hidden_dim, self.output_dim]))\n",
    "        self.b2 = tf.Variable(tf.random_normal([self.output_dim]))\n",
    "        # Network structure. As before, we use tanh for both \n",
    "        # layers. This is not strictly necessary, and TensorFlow\n",
    "        # makes it easier to try different combinations.\n",
    "        self.h = tf.nn.tanh(tf.matmul(self.x, self.W1) + self.b1)    \n",
    "        self.y = tf.nn.softmax(tf.matmul(self.h, self.W2) + self.b2)        \n",
    "        # A place holder for the true labels. None in the first\n",
    "        # dimension allows us to train and evaluate on datasets\n",
    "        # of different size.\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, self.output_dim])\n",
    "        # This defines the objective as one of reducing the \n",
    "        # one-half squared total error. This could easily \n",
    "        # be made into a user-supplied parameter to facilitate\n",
    "        # exploration of other costs. See\n",
    "        # https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#reduction\n",
    "#        cost = tf.reduce_sum(0.5 * (self.y_ - self.y)**tf.constant(2.0))\n",
    "\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(self.y), reduction_indices=[1]))\n",
    "        # Simple GradientDescent (as opposed to the stochastic version\n",
    "        # used by `ShallowNeuralNetwork`). For more options, see\n",
    "        # https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#optimizers\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.eta).minimize(cost)\n",
    "        # TF session initialization:   \n",
    "        init = tf.initialize_all_variables()\n",
    "        self.sess.run(init)\n",
    "        # Train (for larger datasets, the epochs should be batched):\n",
    "        x, y_ = zip(*training_data)\n",
    "        for iteration in range(self.maxiter):\n",
    "            for bat in range(len(x)/100):\n",
    "                #print bat\n",
    "                x_bat = x[bat*100:bat*100 + 100]\n",
    "                y_bat = y_[bat*100:bat*100 + 100]\n",
    "                self.sess.run(self.optimizer, feed_dict={self.x: x_bat, self.y_: y_bat})\n",
    "            if iteration % 20 == 0:\n",
    "#                    print 'Iteration: ' + str(iteration)\n",
    "                self.get_accuracy(training_data)\n",
    "            #self.loss_hist.append(loss)\n",
    "\n",
    "    def predict(self, ex):\n",
    "        \"\"\"\n",
    "        Prediction for `ex`. This runs the model (forward propagation with\n",
    "        self.x replaced by the single example `ex`).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ex : np.array\n",
    "          Must be featurized as the training data were.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            The predicted outputs, dimension self.output_dim. TensorFlow\n",
    "            assumes self.x is a list of examples and so returns a list of\n",
    "            predictions. Since we're classifying just one, we return the\n",
    "            list's only member.\n",
    "            \n",
    "        \"\"\"\n",
    "        return self.sess.run(self.y, feed_dict={self.x: [ex]})[0]\n",
    "    \n",
    "    def get_accuracy(self, test_data):\n",
    "        x, y_ = zip(*test_data)\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(self.y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print self.sess.run(accuracy, feed_dict={self.x: x, self.y_: y_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "devs = []\n",
    "with open('snli_1.0/snli_1.0_dev.jsonl') as f:\n",
    "    for line in f:\n",
    "        j = json.loads(line)\n",
    "        devs.append(j)\n",
    "        \n",
    "trains = []\n",
    "with open('snli_1.0/snli_1.0_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        j = json.loads(line)\n",
    "        trains.append(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550152\n",
      "10000\n",
      "A woman is walking across the street eating a banana, while a man is following with his briefcase.\n"
     ]
    }
   ],
   "source": [
    "snli = {}\n",
    "snli['dev'] = devs\n",
    "snli['train'] = trains\n",
    "print len(trains)\n",
    "print len(devs)\n",
    "print trains[100]['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42803\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for example in snli['train'] + snli['dev']:\n",
    "    parse1 = example['sentence1_binary_parse'].replace('(', ' ').replace(')', ' ').split()\n",
    "    for word in parse1:\n",
    "        vocabulary.add(word)\n",
    "    parse1 = example['sentence2_binary_parse'].replace('(', ' ').replace(')', ' ').split()\n",
    "    for word in parse1:\n",
    "        vocabulary.add(word)\n",
    "print len(vocabulary)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42803\n"
     ]
    }
   ],
   "source": [
    "glove_home = '../cs224uold2/glove.6B'\n",
    "GLOVE = utils.glove2dict(os.path.join(glove_home, 'glove.6B.50d.txt'))\n",
    "def build_glove_embedding(vocab):\n",
    "    glove_vocab = {}\n",
    "    for x in vocab:\n",
    "        if x in GLOVE:\n",
    "            glove_vocab[x] = np.array(GLOVE[x])\n",
    "        else:\n",
    "            glove_vocab[x] = utils.randvec(50)\n",
    "#    return np.array([GLOVE[x] if x in GLOVE else utils.randvec(50) for x in vocab ])\n",
    "    return glove_vocab\n",
    "\n",
    "glove_embedding = build_glove_embedding(vocabulary)\n",
    "print len(glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_class_ind(c):\n",
    "    if c == 'neutral':\n",
    "        return np.array([0.0, 1.0, 0.0])\n",
    "    elif c == 'entailment':\n",
    "        return np.array([0.0, 0.0, 1.0])\n",
    "    elif c == 'contradiction':\n",
    "        return np.array([1.0, 0.0, 0.0])\n",
    "    else:\n",
    "        print c\n",
    "        raise Exception('WTF just happened')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550152\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "def build_dataset(reader):\n",
    "    dataset = []\n",
    "    for (s1, s2, gold_label) in reader():\n",
    "        #print s1, s2, gold_label\n",
    "        dataset.append((sum_sentence(s1) - sum_sentence(s2), get_class_ind(gold_label)))\n",
    "    return dataset\n",
    "                       \n",
    "dataset['train'] = build_dataset(train_reader)\n",
    "print len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['dev'] = build_dataset(dev_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.58539      2.28632     -2.32719     -0.51508      4.42605      3.120233\n",
      "  -2.7889461   -0.36035      0.70848577   0.060274     1.11412     -0.35686\n",
      "  -0.52587      0.88838      2.3264605   -0.412492    -1.41214      2.28166\n",
      "  -1.979853     0.67196     -1.879842     2.67322     -0.429915     2.11549\n",
      "   0.71129    -10.1074      -2.90883      1.742431     0.841326    -1.734354\n",
      "  16.09        -2.30344     -1.75328      0.21298      1.55362713\n",
      "   0.9529049    0.76468     -1.44993      1.172401    -3.168863    -1.046373\n",
      "   0.66836     -1.945562    -0.846426     1.189435    -1.37259      0.8858249\n",
      "  -3.148662     1.29461     -0.073581  ]\n"
     ]
    }
   ],
   "source": [
    "def sum_sentence(s):\n",
    "    result = np.zeros(glove_embedding['the'].shape)\n",
    "    for c in s.split():\n",
    "        result += glove_embedding[c]\n",
    "    return result\n",
    "\n",
    "print sum_sentence('the boy is a girl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is with the difference feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "0.34209\n",
      "0.38099\n",
      "0.4067\n",
      "0.4177\n",
      "0.42543\n",
      "0.43207\n",
      "0.43776\n",
      "0.44366\n",
      "0.44854\n",
      "0.45085\n",
      "0.45302\n",
      "0.45465\n",
      "0.45623\n",
      "0.45708\n",
      "0.4577\n",
      "0.45831\n",
      "0.45853\n",
      "0.45861\n",
      "0.45955\n",
      "0.45952\n",
      "0.46014\n",
      "0.46055\n",
      "0.46077\n",
      "0.46091\n",
      "0.46141\n",
      "0.4582\n",
      "Learning rate:  0.003\n",
      "0.35233\n",
      "0.4229\n",
      "0.44823\n",
      "0.45689\n",
      "0.45885\n",
      "0.46072\n",
      "0.46164\n",
      "0.46239\n",
      "0.46317\n",
      "0.46419\n",
      "0.46517\n",
      "0.4657\n",
      "0.46661\n",
      "0.46731\n",
      "0.46777\n",
      "0.46848\n",
      "0.46939\n",
      "0.46966\n",
      "0.47068\n",
      "0.47136\n",
      "0.47202\n",
      "0.47287\n",
      "0.47374\n",
      "0.47395\n",
      "0.47465\n",
      "0.4673\n",
      "Learning rate:  0.01\n",
      "0.38221\n",
      "0.45831\n",
      "0.46316\n",
      "0.46731\n",
      "0.47091\n",
      "0.47512\n",
      "0.4773\n",
      "0.47927\n",
      "0.48143\n",
      "0.4838\n",
      "0.48565\n",
      "0.48738\n",
      "0.48881\n",
      "0.49051\n",
      "0.49266\n",
      "0.49322\n",
      "0.49472\n",
      "0.49626\n",
      "0.49788\n",
      "0.49896\n",
      "0.49965\n",
      "0.50131\n",
      "0.50272\n",
      "0.5043\n",
      "0.50549\n",
      "0.4998\n",
      "Learning rate:  0.03\n",
      "0.38267\n",
      "0.46596\n",
      "0.47825\n",
      "0.48445\n",
      "0.48954\n",
      "0.49415\n",
      "0.49967\n",
      "0.50337\n",
      "0.50561\n",
      "0.50876\n",
      "0.51147\n",
      "0.51442\n",
      "0.51851\n",
      "0.51906\n",
      "0.52073\n",
      "0.52254\n",
      "0.52686\n",
      "0.52851\n",
      "0.52999\n",
      "0.5313\n",
      "0.53198\n",
      "0.5332\n",
      "0.53465\n",
      "0.5368\n",
      "0.53685\n",
      "0.511\n",
      "Learning rate:  0.1\n",
      "0.41931\n",
      "0.46984\n",
      "0.49016\n",
      "0.50369\n",
      "0.50963\n",
      "0.51413\n",
      "0.51937\n",
      "0.51763\n",
      "0.52197\n",
      "0.52065\n",
      "0.53098\n",
      "0.53134\n",
      "0.53235\n",
      "0.53281\n",
      "0.53701\n",
      "0.53696\n",
      "0.53884\n",
      "0.53598\n",
      "0.53852\n",
      "0.54047\n",
      "0.54076\n",
      "0.54357\n",
      "0.54191\n",
      "0.54199\n",
      "0.54353\n",
      "0.5164\n",
      "Learning rate:  0.3\n",
      "0.40825\n",
      "0.48573\n",
      "0.50106\n",
      "0.5098\n",
      "0.51691\n",
      "0.51913\n",
      "0.51984\n",
      "0.52272\n",
      "0.52393\n",
      "0.52206\n",
      "0.53021\n",
      "0.53356\n",
      "0.53054\n",
      "0.52643\n",
      "0.52887\n",
      "0.52997\n",
      "0.53107\n",
      "0.5319\n",
      "0.53185\n",
      "0.53036\n",
      "0.5312\n",
      "0.53831\n",
      "0.5384\n",
      "0.53747\n",
      "0.53887\n",
      "0.5178\n"
     ]
    }
   ],
   "source": [
    "for eta in [1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1]:\n",
    "    print 'Learning rate: ', eta\n",
    "    baseline_tfnetwork = TfNeuralNetwork(maxiter=500, hidden_dim=100, eta=eta)\n",
    "#print dataset['train'][0]\n",
    "    baseline_tfnetwork.fit(dataset['train'][:100000])\n",
    "\n",
    "    baseline_tfnetwork.get_accuracy(dataset['dev'][:10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45137\n",
      "0.512754\n",
      "0.52163\n",
      "0.527848\n",
      "0.527446\n",
      "0.530264\n",
      "0.53279\n",
      "0.531286\n",
      "0.53502\n",
      "0.533058\n",
      "0.53417\n",
      "0.533536\n",
      "0.532442\n",
      "0.53608\n",
      "0.535532\n",
      "0.537636\n",
      "0.536706\n",
      "0.53728\n",
      "0.537458\n",
      "0.532756\n",
      "0.540312\n",
      "0.533914\n",
      "0.538384\n",
      "0.538374\n",
      "0.536638\n",
      "0.5359\n"
     ]
    }
   ],
   "source": [
    "baseline_tfnetwork = TfNeuralNetwork(maxiter=500, hidden_dim=100, eta=0.3)\n",
    "\n",
    "baseline_tfnetwork.fit(dataset['train'][:500000])\n",
    "\n",
    "baseline_tfnetwork.get_accuracy(dataset['dev'][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.452196\n",
      "0.48023\n",
      "0.490888\n",
      "0.502426\n",
      "0.507742\n",
      "0.510706\n",
      "0.515684\n",
      "0.515982\n",
      "0.515826\n",
      "0.52047\n",
      "0.524506\n",
      "0.520624\n",
      "0.524036\n",
      "0.526338\n",
      "0.526836\n",
      "0.52515\n",
      "0.528734\n",
      "0.52045\n",
      "0.531844\n",
      "0.531632\n",
      "0.530328\n",
      "0.532806\n",
      "0.5316\n",
      "0.531394\n",
      "0.532868\n",
      "0.53118\n",
      "0.533836\n",
      "0.535378\n",
      "0.535306\n",
      "0.532922\n",
      "0.532522\n",
      "0.53598\n",
      "0.534452\n",
      "0.535654\n",
      "0.534928\n",
      "0.53407\n",
      "0.532534\n",
      "0.53454\n",
      "0.535584\n",
      "0.535726\n",
      "0.533238\n",
      "0.534248\n",
      "0.534026\n",
      "0.534706\n",
      "0.537888\n",
      "0.539582\n",
      "0.534348\n",
      "0.534058\n",
      "0.537516\n",
      "0.536734\n",
      "0.535514\n",
      "0.534788\n",
      "0.535224\n",
      "0.534752\n",
      "0.534562\n",
      "0.536704\n",
      "0.536182\n",
      "0.538338\n",
      "0.536028\n",
      "0.539184\n",
      "0.538316\n",
      "0.535416\n",
      "0.538148\n",
      "0.539262\n",
      "0.537866\n",
      "0.536548\n",
      "0.537444\n",
      "0.537764\n",
      "0.53835\n",
      "0.537948\n",
      "0.536192\n",
      "0.539238\n",
      "0.538582\n",
      "0.536334\n",
      "0.539568\n",
      "0.53638\n",
      "0.539832\n",
      "0.538068\n",
      "0.539262\n",
      "0.538904\n",
      "0.537722\n",
      "0.539344\n",
      "0.537792\n",
      "0.53723\n",
      "0.539128\n",
      "0.536024\n",
      "0.539884\n",
      "0.536572\n",
      "0.536956\n",
      "0.537526\n",
      "0.536762\n",
      "0.532646\n",
      "0.538626\n",
      "0.538774\n",
      "0.537812\n",
      "0.536294\n",
      "0.536594\n",
      "0.53673\n",
      "0.539962\n",
      "0.541066\n",
      "0.5333\n"
     ]
    }
   ],
   "source": [
    "baseline_tfnetwork = TfNeuralNetwork(maxiter=200, hidden_dim=50, eta=0.3)\n",
    "\n",
    "baseline_tfnetwork.fit(dataset['train'][:500000])\n",
    "\n",
    "baseline_tfnetwork.get_accuracy(dataset['dev'][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550152\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "def build_dataset(reader):\n",
    "    dataset = []\n",
    "    for (s1, s2, gold_label) in reader():\n",
    "        #print s1, s2, gold_label\n",
    "        dataset.append((np.concatenate((sum_sentence(s1),sum_sentence(s2))), get_class_ind(gold_label)))\n",
    "    return dataset\n",
    "                       \n",
    "dataset['train'] = build_dataset(train_reader)\n",
    "print len(dataset['train'])\n",
    "print dataset['train'][0][0].shape\n",
    "dataset['dev'] = build_dataset(dev_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39277\n",
      "0.47883\n",
      "0.50646\n",
      "0.513506\n",
      "0.522418\n",
      "0.528482\n",
      "0.535132\n",
      "0.538664\n",
      "0.542118\n",
      "0.545706\n",
      "0.54833\n",
      "0.55167\n",
      "0.553708\n",
      "0.55413\n",
      "0.556608\n",
      "0.556438\n",
      "0.557914\n",
      "0.56065\n",
      "0.561768\n",
      "0.56359\n",
      "0.564596\n",
      "0.565962\n",
      "0.56831\n",
      "0.569742\n",
      "0.570718\n",
      "0.57\n"
     ]
    }
   ],
   "source": [
    "baseline_tfnetwork = TfNeuralNetwork(maxiter=500, hidden_dim=100, eta=0.03)\n",
    "\n",
    "baseline_tfnetwork.fit(dataset['train'][:500000])\n",
    "\n",
    "baseline_tfnetwork.get_accuracy(dataset['dev'][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382264\n",
      "0.449592\n",
      "0.494022\n",
      "0.510698\n",
      "0.521378\n",
      "0.531202\n",
      "0.53661\n",
      "0.537982\n",
      "0.546754\n",
      "0.550888\n",
      "0.552598\n",
      "0.55207\n",
      "0.553916\n",
      "0.558788\n",
      "0.55859\n",
      "0.56077\n",
      "0.562576\n",
      "0.564492\n",
      "0.566138\n",
      "0.569358\n",
      "0.56877\n",
      "0.571264\n",
      "0.57104\n",
      "0.572082\n",
      "0.574118\n",
      "0.57512\n",
      "0.575934\n",
      "0.576546\n",
      "0.577794\n",
      "0.577962\n",
      "0.577126\n",
      "0.577384\n",
      "0.57799\n",
      "0.578698\n",
      "0.57836\n",
      "0.580452\n",
      "0.57923\n",
      "0.57938\n",
      "0.581084\n",
      "0.579874\n",
      "0.57982\n",
      "0.581392\n",
      "0.579782\n",
      "0.580666\n",
      "0.580502\n",
      "0.58144\n",
      "0.581786\n",
      "0.580994\n",
      "0.580454\n",
      "0.577694\n",
      "0.580196\n",
      "0.58103\n",
      "0.582168\n",
      "0.58024\n",
      "0.580028\n",
      "0.58104\n",
      "0.5806\n",
      "0.581546\n",
      "0.58024\n",
      "0.579312\n",
      "0.58073\n",
      "0.580946\n",
      "0.580582\n",
      "0.581316\n",
      "0.582918\n",
      "0.581982\n",
      "0.583378\n",
      "0.583212\n",
      "0.582436\n",
      "0.582498\n",
      "0.582336\n",
      "0.58108\n",
      "0.58409\n",
      "0.58369\n",
      "0.582508\n",
      "0.5883\n"
     ]
    }
   ],
   "source": [
    "baseline_tfnetwork = TfNeuralNetwork(maxiter=1500, hidden_dim=100, eta=0.03)\n",
    "\n",
    "baseline_tfnetwork.fit(dataset['train'][:500000])\n",
    "\n",
    "baseline_tfnetwork.get_accuracy(dataset['dev'][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "import nltk\n",
    "# exclude = set(string.punctuation)\n",
    "\n",
    "# an alternative method: still doesn't seem to give enough vocab. below (somewhere) i just take the vocab from the\n",
    "# build_rnn_dataset output\n",
    "vc = nltk.word_tokenize(' '.join([sentence['sentence1']+ ' '+sentence['sentence2'] for sentence in (snli['train'] + snli['dev'])]))\n",
    "\n",
    "# vc = set()\n",
    "# for sentence in snli['train'][:10000]:\n",
    "#     things = sentence['sentence1']\n",
    "# #     for thing in things:\n",
    "# #         thing = ''.join(ch for ch in thing if ch not in exclude)\n",
    "# #         vc.add(thing)\n",
    "#     \n",
    "# #     for thing in nltk.word_tokenize(things):\n",
    "# #         vc.add(thing)\n",
    "# # #     for thing in things:\n",
    "# # #         thing = ''.join(ch for ch in thing if ch not in exclude)\n",
    "# # #         vc.add(thing)\n",
    "# #     for thing in nltk.word_tokenize(things):\n",
    "# #         vc.add(thing)\n",
    "        \n",
    "\n",
    "snli['vocab'] = list(set(vc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-d216c3510a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print len(snli_sample['train'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print snli['vocab']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr2tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence2_binary_parse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "\n",
    "print (str2tree(snli['train'][0]['sentence2_binary_parse'])).subtrees()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snli_reader(sample):\n",
    "    \"\"\"Reader for SNLI data. `sample` just needs to be an iterator over\n",
    "    the SNLI JSONL files. For this notebook, it will always be \n",
    "    `snli_sample`, but, for example, the following should work for the \n",
    "    corpus files:\n",
    "    \n",
    "    import json    \n",
    "    def sample(src_filename):\n",
    "        for line in open(src_filename):\n",
    "            yield json.loads(line)\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        (tree1, tree2, label), where the trees are from `str2tree` and\n",
    "        label is in `LABELS` above.\n",
    "      \n",
    "    \"\"\"\n",
    "    for d in sample:\n",
    "        if d['gold_label'] == '-':\n",
    "            d['gold_label'] = d['annotator_labels'][0]\n",
    "        yield (d['sentence1_binary_parse'].replace('(', ' ').replace(')', ' '), \n",
    "               d['sentence2_binary_parse'].replace('(', ' ').replace(')', ' '),\n",
    "               d['gold_label'])\n",
    "        \n",
    "def train_reader():\n",
    "    \"\"\"Convenience function for reading just the training data.\"\"\"\n",
    "    return snli_reader(snli['train'])\n",
    "\n",
    "def dev_reader():\n",
    "    \"\"\"Convenience function for reading just the dev data.\"\"\"\n",
    "    return snli_reader(snli['dev'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2tree(s):\n",
    "    \"\"\"Map str `s` to an `nltk.tree.Tree` instance. The assumption is that \n",
    "    `s` represents a standard Penn-style tree.\"\"\"\n",
    "    return Tree.fromstring(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn_dataset(reader):\n",
    "    \"\"\"Build RNN datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader\n",
    "        SNLI iterator like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        The first member of each tuple is a list of strings (the\n",
    "        concatenated leaves) and the second is an np.array \n",
    "        (dimension 3) with a single 1 for the true class and 0s\n",
    "        in the other two positions\n",
    "       \n",
    "    \"\"\"    \n",
    "    dataset = []\n",
    "    for (t1, t2, label) in reader():\n",
    "        seq = t1.leaves() + t2.leaves()\n",
    "        y_ = np.zeros(3)\n",
    "        if label == '-':\n",
    "            label = 'neutral'\n",
    "        y_[LABELS.index(label)] = 1.0\n",
    "        dataset.append((seq, y_))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model_evaluation(mod, assess, labels=LABELS):\n",
    "    \"\"\"Asssess a trained `ClassifierRNN`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mod : `ClassifierRNN`\n",
    "        Should be a model trained on data in the same format as\n",
    "        `assess`.\n",
    "    \n",
    "    assess : list\n",
    "        A list of (seq, label) pairs, where seq is a sequence of\n",
    "        words and label is a one-hot vector giving the label.        \n",
    "    \n",
    "    \"\"\"    \n",
    "    # Assessment:\n",
    "    gold = []\n",
    "    predictions = []    \n",
    "    for seq, y_ in assess:\n",
    "        # The gold labels are vectors. Get the index of the single 1\n",
    "        # and look up its string in `LABELS`:\n",
    "        gold.append(labels[np.argmax(y_)])\n",
    "        # `predict` returns the index of the highest score.\n",
    "        p = mod.predict(seq) \n",
    "        predictions.append(labels[p])\n",
    "    # Report:\n",
    "    return classification_report(gold, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_experiment(\n",
    "        train_inp,\n",
    "        dev_inp,\n",
    "        vocab, \n",
    "        embedding, \n",
    "        hidden_dim, \n",
    "        eta, \n",
    "        maxiter):\n",
    "    \"\"\"Classifier RNN experiments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : list of str\n",
    "        Must contain every word we'll encounter in training or assessment.\n",
    "        \n",
    "    embedding : np.array\n",
    "        Embedding matrix for `vocab`. The ith row gives the input \n",
    "        representation for the ith member of vocab. Thus, `embedding`\n",
    "        must have the same row count as the length of vocab. Its\n",
    "        columns can be any length. (That is, the input word \n",
    "        representations can be any length.)\n",
    "        \n",
    "    hidden_dim : int (default: 10)\n",
    "        Dimensionality of the hidden representations. This is a\n",
    "        parameter to `ClassifierRNN`.\n",
    "        \n",
    "    eta : float (default: 0.05)\n",
    "        The learning rate. This is a parameter to `ClassifierRNN`.       \n",
    "        \n",
    "    maxiter : int (default: 10)\n",
    "        Maximum number of training epochs. This is a parameter \n",
    "        to `ClassifierRNN`.       \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted `sklearn` `classification_report`.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Training:\n",
    "    train = build_rnn_dataset(train_inp)       \n",
    "    mod = ClassifierRNN(\n",
    "        vocab, \n",
    "        embedding, \n",
    "        hidden_dim=hidden_dim, \n",
    "        eta=eta,\n",
    "        maxiter=maxiter)\n",
    "    mod.fit(train)    \n",
    "    # Assessment:\n",
    "    assess = build_rnn_dataset(dev_inp) \n",
    "    return rnn_model_evaluation(mod, assess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc = set()\n",
    "\n",
    "for x in build_rnn_dataset(train_reader):\n",
    "    vc = vc.union(set(x[0]))\n",
    "        \n",
    "for x in build_rnn_dataset(dev_reader):\n",
    "    vc = vc.union(set(x[0]))\n",
    "        \n",
    "snli['vocab'] = list(vc)\n",
    "# vc = set()\n",
    "# print [x[0] for x in build_rnn_dataset(train_reader)[:5]]\n",
    "# a = [[u'A', u'person', u'on', u'a', u'horse', u'jumps', u'over', u'a', u'broken', u'down', u'airplane', u'.', u'A', u'person', u'is', u'training', u'his', u'horse', u'for', u'a', u'competition', u'.'], [u'A', u'person', u'on', u'a', u'horse', u'jumps', u'over', u'a', u'broken', u'down', u'airplane', u'.', u'A', u'person', u'is', u'at', u'a', u'diner', u',', u'ordering', u'an', u'omelette', u'.'], [u'A', u'person', u'on', u'a', u'horse', u'jumps', u'over', u'a', u'broken', u'down', u'airplane', u'.', u'A', u'person', u'is', u'outdoors', u',', u'on', u'a', u'horse', u'.'], [u'Children', u'smiling', u'and', u'waving', u'at', u'camera', u'They', u'are', u'smiling', u'at', u'their', u'parents'], [u'Children', u'smiling', u'and', u'waving', u'at', u'camera', u'There', u'are', u'children', u'present']]\n",
    "# for x in a:\n",
    "#     print x\n",
    "#     vc = vc.union(x)\n",
    "# print vc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finished epoch 1 of 1; error is 1.2139977093"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.33      0.50      0.40      3278\n",
      " entailment       0.33      0.50      0.40      3329\n",
      "    neutral       0.00      0.00      0.00      3393\n",
      "\n",
      "avg / total       0.22      0.33      0.26     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = snli['vocab']\n",
    "# for seq,labels in training_data:\n",
    "#     print 1\n",
    "#     break\n",
    "# vocab = snli_sample['vocab']\n",
    "# Random embeddings of dimension 10:\n",
    "randvec_embedding = np.array([utils.randvec(10) for w in vocab])\n",
    "\n",
    "# A small network, trained for just a few epochs to see how things look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "def train_reader_sample():\n",
    "    return snli_reader(snli_sample['train'])\n",
    "def dev_reader_sample(): \n",
    "    return snli_reader(snli_sample['dev'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "glove_home = '/Users/reuben/Documents/glove.6B'\n",
    "GLOVE = utils.glove2dict(os.path.join(glove_home, 'glove.6B.50d.txt'))\n",
    "\n",
    "\n",
    "def build_glove_embedding(vocab):\n",
    "    return np.array([GLOVE[x] if x in GLOVE else utils.randvec(50) for x in vocab ])\n",
    "\n",
    "print 1\n",
    "glove_embedding = build_glove_embedding(snli_sample['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 1.11585145787"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.50      0.01      0.01      3278\n",
      " entailment       0.33      0.98      0.50      3329\n",
      "    neutral       0.46      0.03      0.06      3393\n",
      "\n",
      "avg / total       0.43      0.34      0.19     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print rnn_experiment(train_reader,\n",
    "                     dev_reader,\n",
    "                     snli['vocab'], \n",
    "                     build_glove_embedding(snli['vocab']), \n",
    "                     hidden_dim=10, \n",
    "                     eta=0.01, \n",
    "                     maxiter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cross_product_phi(t1, t2):\n",
    "    \"\"\"Basis for cross-product features. This tends to produce pretty \n",
    "    dense representations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1, t2 : `nltk.tree.Tree`\n",
    "        As given by `str2tree`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "        Maps each (w1, w2) in the cross-product of `t1.leaves()` and \n",
    "        `t2.leaves()` to its count. This is a multi-set cross-product\n",
    "        (repetitions matter).\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter([(w1, w2) for w1, w2 in itertools.product(t1.leaves(), t2.leaves())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_linear_classifier_dataset(\n",
    "        reader,\n",
    "        phi, \n",
    "        vectorizer=None):\n",
    "    \"\"\"Create a dataset for training classifiers using `sklearn`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader\n",
    "        An SNLI iterator like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : feature function\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    vectorizer : `sklearn.feature_extraction.DictVectorizer`   \n",
    "        If this is None, then a new `DictVectorizer` is created and\n",
    "        used to turn the list of dicts created by `phi` into a \n",
    "        feature matrix. This happens when we are training.\n",
    "              \n",
    "        If this is not None, then it's assumed to be a `DictVectorizer` \n",
    "        and used to transform the list of dicts. This happens in \n",
    "        assessment, when we take in new instances and need to \n",
    "        featurize them as we did in training.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict with keys 'X' (the feature matrix), 'y' (the list of\n",
    "        labels), 'vectorizer' (the `DictVectorizer`), and \n",
    "        'raw_examples' (the original tree pairs, for error analysis).\n",
    "    \n",
    "    \"\"\"\n",
    "    feat_dicts = []\n",
    "    labels = []\n",
    "    raw_examples = []\n",
    "    for t1, t2, label in reader():\n",
    "        d = phi(t1, t2)\n",
    "        feat_dicts.append(d)\n",
    "        labels.append(label)   \n",
    "        raw_examples.append((t1, t2))\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    else:\n",
    "        feat_matrix = vectorizer.transform(feat_dicts)\n",
    "    return {'X': feat_matrix, \n",
    "            'y': labels, \n",
    "            'vectorizer': vectorizer, \n",
    "            'raw_examples': raw_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `sklearn.linear.model.LogisticRegression`. This is also \n",
    "    called a Maximum Entropy (MaxEnt) Classifier, which is more fitting \n",
    "    for the multiclass case.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.\n",
    "        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    `sklearn.linear.model.LogisticRegression`\n",
    "        A trained `LogisticRegression` instance.\n",
    "    \n",
    "    \"\"\"\n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_classifier_experiment(\n",
    "        train_reader,\n",
    "        assess_reader,\n",
    "        phi,\n",
    "        train_func=fit_maxent_classifier):  \n",
    "    \"\"\"Runs experiments on our SNLI fragment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_reader, assess_reader\n",
    "        SNLI iterators like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : feature function (default: `word_overlap_phi`)\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    train_func : model wrapper (default: `fit_maxent_classifier`)\n",
    "        Any function that takes a feature matrix and a label list\n",
    "        as its values and returns a fitted model with a `predict`\n",
    "        function that operates on feature matrices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted `classification_report` from `sklearn`.\n",
    "        \n",
    "    \"\"\"\n",
    "    train = build_linear_classifier_dataset(train_reader, phi)\n",
    "    assess = build_linear_classifier_dataset(assess_reader, phi, vectorizer=train['vectorizer'])\n",
    "    print 1\n",
    "\n",
    "    mod = fit_maxent_classifier(train['X'], train['y'])\n",
    "    predictions = mod.predict(assess['X'])\n",
    "    return classification_report(assess['y'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-746e2cd07d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_classifier_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_reader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_cross_product_phi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-247-5faa0aa25448>\u001b[0m in \u001b[0;36mlinear_classifier_experiment\u001b[0;34m(train_reader, assess_reader, phi, train_func)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_maxent_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-235-4fbaad199511>\u001b[0m in \u001b[0;36mfit_maxent_classifier\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reuben/Downloads/ENTER/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 self.max_iter, self.tol, self.random_state)\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reuben/Downloads/ENTER/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         epsilon)\n\u001b[0m\u001b[1;32m    917\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(linear_classifier_experiment(train_reader,dev_reader,word_cross_product_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
